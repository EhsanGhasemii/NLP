{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97b4a60c-1d81-445b-a7ed-ffff3078d4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from data import SpeechDataset, SpeechDataLoader, featurelen, cer, wer\n",
    "from uyghur import uyghur_latin\n",
    "from tqdm import tqdm\n",
    "from UModel import UModel\n",
    "\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "class CustOpt:\n",
    "    def __init__(self, params, datalen, lr, min_lr = None):\n",
    "        if min_lr is None:\n",
    "            min_lr = lr\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(params, lr=lr, weight_decay=0.000001)  #, weight_decay=0.000001\n",
    "        self._step = 0\n",
    "        self.scheduler = CosineAnnealingLR(self.optimizer,T_max=datalen, eta_min = min_lr)\n",
    "\n",
    "    def step(self):\n",
    "        self.optimizer.step()\n",
    "        self.scheduler.step()\n",
    "        rate = self.scheduler.get_last_lr()[0]\n",
    "        return rate\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "#outputs format = B x F x T\n",
    "def calctc_loss(outputs, targets, output_lengths, target_lengths):\n",
    "    loss = F.ctc_loss(outputs.permute(2,0,1).contiguous(), targets, output_lengths, target_lengths, blank = uyghur_latin.pad_idx, reduction='mean',zero_infinity=True)\n",
    "    return loss\n",
    "\n",
    "def validate(model, valid_loader):\n",
    "    chars = 0\n",
    "    words = 0\n",
    "    e_chars = 0\n",
    "    e_words = 0\n",
    "    avg_loss = 0\n",
    "    iter_cnt = 0\n",
    "    msg = \"\"\n",
    "    \n",
    "    cer_val = 0.0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        tlen = len(valid_loader)\n",
    "        vbar = tqdm(iter(valid_loader), leave=True, total=tlen)\n",
    "        for inputs, targets, input_lengths, target_lengths, _ in vbar:\n",
    "\n",
    "            inputs  = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs, output_lengths = model(inputs, input_lengths)\n",
    "            loss = calctc_loss(outputs, targets, output_lengths, target_lengths)\n",
    "            preds   = model.greedydecode(outputs, output_lengths)\n",
    "            targets = [uyghur_latin.decode(target) for target in targets]\n",
    "            \n",
    "            for pred, src in zip(preds, targets):\n",
    "                e_char_cnt, char_cnt = cer(pred,src)\n",
    "                e_word_cnt, word_cnt = wer(pred, src)\n",
    "                e_chars += e_char_cnt\n",
    "                e_words += e_word_cnt\n",
    "\n",
    "                chars += char_cnt\n",
    "                words += word_cnt\n",
    "\n",
    "            iter_cnt += 1\n",
    "            avg_loss +=loss.item()\n",
    "\n",
    "            msg = f\"  VALIDATION: [CER:{e_chars/chars:.2%} ({e_chars}/{chars} letters) WER:{e_words/words:.2%} ({e_words}/{words} words), Avg loss:{avg_loss/iter_cnt:4f}]\"\n",
    "            vbar.set_description(msg)\n",
    "\n",
    "        vbar.close()\n",
    "\n",
    "        cer_val = e_chars/chars\n",
    "\n",
    "        with open(log_name,'a', encoding='utf-8') as fp:\n",
    "            fp.write(msg+\"\\n\")\n",
    "\n",
    "        #Print Last 3 validation results\n",
    "        result =\"\"\n",
    "        result_cnt = 0\n",
    "        for pred, src in zip(preds, targets):\n",
    "            e_char_cnt, char_cnt = cer(pred,src)\n",
    "            e_word_cnt, word_cnt = wer(pred, src)\n",
    "            result += f\"   O:{src}\\n\"\n",
    "            result += f\"   P:{pred}\\n\"\n",
    "            result += f\"     CER: {e_char_cnt/char_cnt:.2%} ({e_char_cnt}/{char_cnt} letters), WER: {e_word_cnt/word_cnt:.2%} ({e_word_cnt}/{word_cnt} words)\\n\"\n",
    "            result_cnt += 1\n",
    "            if result_cnt >= 3:\n",
    "                break\n",
    "        \n",
    "        print(result)\n",
    "        return cer_val\n",
    "\n",
    "\n",
    "def train(model, train_loader):\n",
    "    total_loss = 0\n",
    "    iter_cnt = 0\n",
    "    msg =''\n",
    "    model.train()\n",
    "    pbar = tqdm(iter(train_loader), leave=True, total=mini_epoch_length)\n",
    "    for data in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        inputs, targets, input_lengths, target_lengths, _ = data\n",
    "        inputs  = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        outputs, output_lengths = model(inputs, input_lengths)\n",
    "        loss = calctc_loss(outputs, targets, output_lengths, target_lengths)\n",
    "        loss.backward()\n",
    "\n",
    "        lr = optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        iter_cnt += 1\n",
    "\n",
    "        msg = f'[LR: {lr: .7f} Loss: {loss.item(): .5f}, Avg loss: {(total_loss/iter_cnt): .5f}]'\n",
    "        pbar.set_description(msg)\n",
    "        if iter_cnt > mini_epoch_length:\n",
    "            break\n",
    "        \n",
    "    pbar.close()\n",
    "    with open(log_name,'a', encoding='utf-8') as fp:\n",
    "        msg = f'Epoch[{(epoch+1):d}]:\\t{msg}\\n'\n",
    "        fp.write(msg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b9b3096d-aebe-4760-8334-af32c5adc736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "other.tsv\n"
     ]
    }
   ],
   "source": [
    "prefix_path = \"../datasets/cv-corpus-17.0-delta-2024-03-15/fa/\"\n",
    "prefix_path2 = \"clips_wav/\"\n",
    "train_file = 'other.tsv'\n",
    "\n",
    "print(train_file)\n",
    "\n",
    "with open(prefix_path + train_file,encoding='utf_8_sig') as f:\n",
    "    lines = f.readlines()                               # len(lines): 9923, list\n",
    "                                                        # line[0]: \".wav \\t script\"\n",
    "\n",
    "idxs  = []\n",
    "idxs2 = []\n",
    "for x in lines:\n",
    "    _, path, _, sentence = x.strip().split(\"\\t\")[0:4]\n",
    "    path = path.strip().split(\".\")[0]\n",
    "    path = prefix_path + prefix_path2 + path + \".wav\"\n",
    "    # print(path)\n",
    "    # print(sentence)\n",
    "    \n",
    "    if os.path.exists(path):\n",
    "        line = []\n",
    "        line.append(path)\n",
    "        idxs2.append(sentence)\n",
    "        char_indx = uyghur_latin.encode(sentence)\n",
    "        line.append(char_indx)\n",
    "        idxs.append(line)                           # len(idx): 9923, list\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a54aff4f-a993-4cc1-8b34-edc833a324d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6208\n",
      "6209\n"
     ]
    }
   ],
   "source": [
    "print(len(idxs))\n",
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d88d7561-a133-42ca-8d47-84db4d0ee565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n",
      "['<pad>', '<sos>', '<eos>', 'آ', 'ئ', 'ا', 'ب', 'پ', 'ت', 'ث', 'ج', 'چ', 'ح', 'خ', 'د', 'ذ', 'ر', 'ژ', 'س', 'ش', 'ص', 'ض', 'ط', 'ظ', 'ع', 'غ', 'ف', 'ق', 'ک', 'گ', 'ل', 'م', 'ن', 'و', 'ه', 'ی', 'ز', ' ']\n"
     ]
    }
   ],
   "source": [
    "print(uyghur_latin.vocab_size)\n",
    "print(uyghur_latin.vocab_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "141c9774-c1c3-46e8-9f75-1437604d0ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['../datasets/cv-corpus-17.0-delta-2024-03-15/fa/clips_wav/common_voice_fa_39591341.wav', [5, 26, 18, 16, 37, 18, 16, 16, 19, 8, 34, 37, 14, 5, 16, 35]], ['../datasets/cv-corpus-17.0-delta-2024-03-15/fa/clips_wav/common_voice_fa_39591342.wav', [13, 33, 5, 6, 19, 37, 32, 31, 35, 6, 16, 14, 37, 33, 37, 5, 26, 28, 5, 16, 37, 19, 33, 16, 35, 14, 34, 35, 37, 5, 33, 37, 31, 8, 33, 10, 34, 37, 31, 18, 5, 6, 27, 34, 35, 37, 26, 16, 14, 5, 37, 6, 33, 14]], ['../datasets/cv-corpus-17.0-delta-2024-03-15/fa/clips_wav/common_voice_fa_39591343.wav', [5, 33, 37, 16, 5, 37, 14, 16, 37, 28, 30, 35, 18, 5, 35, 37, 20, 33, 31, 24, 34, 37, 14, 26, 32, 37, 28, 16, 14, 32, 14]]]\n"
     ]
    }
   ],
   "source": [
    "print(idxs[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fcadb9fa-1245-466f-8d0e-960346f1c2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 33, 30, 37, 13, 33, 14, 37, 16, 5, 37, 14, 16, 37, 18, 34, 5, 31, 37, 18, 16, 31, 5, 35, 34, 37, 29, 15, 5, 16, 35, 37, 28, 16, 14, 32]\n",
      "پول خود را در سهام سرمایه گذاری کردن\n",
      "پول خود را در سهام سرمایه گذاری کردن\n"
     ]
    }
   ],
   "source": [
    "num = 63\n",
    "test_case = idxs[num][1]\n",
    "test_case_raw = idxs2[num]\n",
    "print(test_case)\n",
    "print(uyghur_latin.decode(test_case))\n",
    "print(test_case_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f99cada-7428-4b2e-bce0-fa394c42101b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f330ffa2-833c-4467-b939-fdbc6bbf7855",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ff2518-7756-44b5-a747-5adaca895cf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
